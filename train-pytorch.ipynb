{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, MultiStepLR\n",
    "import torchvision\n",
    "from torchvision import transforms, utils, datasets\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import PIL\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "import gdown\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#if '/opt/ros/kinetic/lib/python2.7/dist-packages' in sys.path:\n",
    "#    sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30\n",
    "NUM_WROKERS = 8\n",
    "INPUT_IMG_SIZE = (101, 101)\n",
    "EPOCH = 3000               # train the training data n times, to save time, we just train 1 epoch\n",
    "LR = 0.01               # learning rate\n",
    "DATASET_ROOT = './dataset/joystick'\n",
    "MODELS_ROOT = './models'\n",
    "CLASSES = np.loadtxt('./class_id.txt', str, delimiter='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions: \n",
    "dataset visualization and image loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_img(batch_data):   \n",
    "    # show images\n",
    "    imgs = torchvision.utils.make_grid(batch_data)\n",
    "    imgs = imgs / 2 + 0.5     # unnormalize\n",
    "    npimgs = imgs.numpy()\n",
    "    plt.rcParams['figure.figsize'] = [12, 5]\n",
    "    plt.imshow(np.transpose(npimgs, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, classes):\n",
    "    img_paths = []\n",
    "    labels = []\n",
    "    for class_id, class_name in enumerate(classes):\n",
    "        class_folder = os.path.join(folder, class_name)\n",
    "        for filename in os.listdir(class_folder):\n",
    "            filename.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "            img_paths.append(os.path.join(class_folder, filename))\n",
    "            labels.append(class_id)\n",
    "    return img_paths, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom pytorch dataset class for Trailnet (load dataset from txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrailnetDataset(Dataset):\n",
    "    def __init__(self, datalist_filename):\n",
    "        classes = CLASSES\n",
    "        \n",
    "        self.img_list = []\n",
    "        self.label_list = []\n",
    "        \n",
    "        txt = np.loadtxt(datalist_filename, str, delimiter='\\n')\n",
    "        for line in txt:\n",
    "            p, l  = line.split()\n",
    "            self.img_list.append(p)\n",
    "            self.label_list.append(l)\n",
    "        \n",
    "        print( '********** Dataset Info start **********\\n')\n",
    "        print('Source: ', datalist_filename) \n",
    "        print('Output classes: ', classes) \n",
    "        print( 'Amount of images: ', len(txt))\n",
    "        print('\\n*********** Dataset Info end ***********\\n') \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        self.data_transform = transforms.Compose([ \n",
    "                                transforms.Resize(INPUT_IMG_SIZE), \\\n",
    "                                transforms.ToTensor(), \\\n",
    "                                transforms.Normalize(mean=[0.5, 0.5, 0.5], \\\n",
    "                                                     std=[1, 1, 1]), \\\n",
    "                                ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def collect_folders_from_dataset(self, dataset_root, classes):\n",
    "        # Implement by BFS\n",
    "        search_list = [dataset_root, ]\n",
    "        dataset_folders = [] \n",
    "        while len(search_list) != 0:\n",
    "            root = search_list.pop(0)\n",
    "            if set(os.listdir(root)) == set(classes):\n",
    "                dataset_folders.append(root)\n",
    "            else:\n",
    "                for folder in os.listdir(root):\n",
    "                    path = os.path.join(root, folder)\n",
    "                    if os.path.isdir(path):\n",
    "                        search_list.append(path)\n",
    "        return dataset_folders\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # print self.img_list[index]\n",
    "        # Select sample, then load data and get label\n",
    "        path = self.img_list[index]\n",
    "        img_raw = self.default_loader(path)\n",
    "        x = self.data_transform(img_raw)\n",
    "        z = float(self.label_list[index])\n",
    "        y = int((z/8.5+1)*7)\n",
    "  \n",
    "        return x, y, path\n",
    "    \n",
    "    def pil_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            with PIL.Image.open(f) as img:\n",
    "                return img.convert('RGB')\n",
    "\n",
    "    def accimage_loader(self, path):\n",
    "        try:\n",
    "            return accimage.Image(path)\n",
    "        except IOError:\n",
    "            # Potentially a decoding problem, fall back to PIL.Image\n",
    "            return pil_loader(path)\n",
    "\n",
    "    def default_loader(self, path):\n",
    "        if torchvision.get_image_backend() == 'accimage':\n",
    "            return self.accimage_loader(path)\n",
    "        else:\n",
    "            return self.pil_loader(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / validation dataset declaration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = TrailnetDataset(datalist_filename='./dataset/train.txt')\n",
    "val_dataset = TrailnetDataset(datalist_filename='./dataset/test.txt')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WROKERS, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WROKERS, shuffle=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, model, criterion, optimizer, device, debug_steps=100, epoch=-1):\n",
    "    model.train(True)\n",
    "    running_loss = 0.0\n",
    "    running_regression_loss = 0.0\n",
    "    running_classification_loss = 0.0\n",
    "    model.to(device)\n",
    "    for i, data in enumerate(loader):\n",
    "        images, labels, _path = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "\n",
    " \n",
    "        loss = criterion(outputs, labels)  # TODO CHANGE BOXES\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i and i % debug_steps == 0:\n",
    "            avg_loss = loss / debug_steps\n",
    "            # logging.info(\"Epoch: {}, Step: {}, Average Loss: {:.4f}\".format(epoch, i, avg_loss))\n",
    "            print( \"Epoch: {}, Step: {}, Average Loss: {:.6f}\".format(epoch, i, avg_loss))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "\n",
    "def test(loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    num = 0\n",
    "    for _, data in enumerate(loader):\n",
    "        images, labels, _path = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        num += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet 15 class output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 15),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TrailNet 15 class output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrailNet15Class(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(TrailNet15Class, self).__init__()\n",
    "#         # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "#         # kernel\n",
    "#         self.conv1 = nn.Conv2d(3, 32, 4)\n",
    "#         self.pool1 = nn.MaxPool2d((2, 2), stride=2)\n",
    "#         self.conv2 = nn.Conv2d(32, 32, 4)\n",
    "#         self.pool2 = nn.MaxPool2d((2, 2), stride=2)\n",
    "#         self.conv3 = nn.Conv2d(32, 32, 4)\n",
    "#         self.pool3 = nn.MaxPool2d((2, 2), stride=2)\n",
    "#         self.conv4 = nn.Conv2d(32, 32, 4, padding=(2, 2))\n",
    "#         self.pool4 = nn.MaxPool2d((2, 2), stride=2)\n",
    "#         # an affine operation: y = Wx + b\n",
    "#         self.fc1 = nn.Linear(800, 200)\n",
    "#         self.fc2 = nn.Linear(200, 15)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool1(self.conv1(x))\n",
    "#         x = self.pool2(self.conv2(x))\n",
    "#         x = self.pool3(self.conv3(x)) \n",
    "#         x = self.pool4(self.conv4(x))    \n",
    "#         # print 'x size: ', x.size()   \n",
    "#         x = x.view(-1, self.num_flat_features(x))\n",
    "#         # print 'x size: ', x.size()   \n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "#     def num_flat_features(self, x):\n",
    "#         size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "#         num_features = 1\n",
    "#         for s in size:\n",
    "#             num_features *= s\n",
    "#         return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = TrailNet15Class()\n",
    "model = AlexNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_epoch = -1\n",
    "validation_epochs = 3\n",
    "\n",
    "params = model.parameters()\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_folder = './models'\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params, lr=1e-3, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, 120, last_epoch=last_epoch)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for epoch in range(last_epoch + 1, EPOCH):\n",
    "    scheduler.step()\n",
    "    train(train_loader, model, criterion, optimizer, \\\n",
    "          device=DEVICE, debug_steps=50, epoch=epoch)\n",
    "\n",
    "    if epoch % validation_epochs == 0 or epoch == EPOCH - 1:\n",
    "        val_loss = test(val_loader, model, criterion, DEVICE)\n",
    "        # logging.info(\"Epoch: {}, \".format(epoch) + \\\n",
    "        #            \"Validation Loss: {:.4f}\".format(val_loss))\n",
    "        print(\"Epoch: {}, Validation Loss: {:.6f}\".format(epoch, val_loss)) \n",
    "        model_path = os.path.join(checkpoint_folder, \\\n",
    "                                  \"{}_epoch{}_loss{:.5f}.pth\".format('AlexNet_15class', epoch, val_loss))\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        # logging.info(\"Saved model {}\".format(model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
